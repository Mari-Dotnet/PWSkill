{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "406be045-e63f-468d-a42f-a49a41bc57a2",
   "metadata": {},
   "source": [
    "#### Q1. What is an activation function in the context of artificial neural networks?\n",
    "- An Activation Function decides whether a neuron should be activated or not. This means that it will decide whether the neuron's input to the network is important or not in the process of prediction using simpler mathematical operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e45292-84ec-4859-9891-05eba4c00c95",
   "metadata": {},
   "source": [
    "#### Q2. What are some common types of activation functions used in neural networks?\n",
    "- Relu\n",
    "- Sigmoid\n",
    "- LRelu\n",
    "- Tanh\n",
    "- softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c6ce10-67f2-4593-9d10-ed54c46864f8",
   "metadata": {},
   "source": [
    "#### Q3. How do activation functions affect the training process and performance of a neural network?\n",
    "- An Activation Function decides whether a neuron should be activated or not. This means that it will decide whether the neuron's input to the network is important or not in the process of prediction using simpler mathematical operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af33bc3a-21f6-4ce0-b45c-69ef741b82b0",
   "metadata": {},
   "source": [
    "#### Q4. How does the sigmoid activation function work? What are its advantages and disadvantages?\n",
    "\n",
    "sigmoid function: 1.0 / (1 + np.exp(-z))\n",
    "\n",
    "- Advantages of Sigmoid Function: -\n",
    "    - It provides Smooth gradient which helps us in preventing “jumps” in output values.\n",
    "    - Output values bound between 0 and 1, normalizing the output of each neuron.\n",
    "    - It provides clear predictions, i.e. very close to 1 or 0 which helps us to improve model performance.\n",
    "\n",
    "- Disadvantages of Sigmoid functions:\n",
    "    - It is most prone to gradient vanishing problem.\n",
    "    - Function output is not zero-centred.\n",
    "    - Power operations are relatively time-consuming which increases model complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c415b13-fbda-4c79-905b-f4c15d1bde59",
   "metadata": {},
   "source": [
    "#### Q5.What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?\n",
    "- ReLU is h=max(0,a)\n",
    "- One major benefit is the reduced likelihood of the gradient to vanish. This arises when a>0\n",
    ".In this regime the gradient has a constant value. In contrast, the gradient of sigmoids becomes increasingly small as the absolute value of x increases. The constant gradient of ReLUs results in faster learning.\n",
    "- Efficiency: ReLu is faster to compute than the sigmoid function, and its derivative is faster to compute. This makes a significant difference to training and inference time for neural networks: only a constant factor, but constants can matter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcea41a4-b6f9-4242-aa26-71491c5d21cd",
   "metadata": {},
   "source": [
    "#### Q6. What are the benefits of using the ReLU activation function over the sigmoid function?\n",
    "- Historical accident: we discovered ReLu in the early days before we knew about those tricks, so in the early days ReLu was the only choice that worked, and everyone had to use it. And now that everyone uses it, it is a safe choice and people keep using it.\n",
    "\n",
    "- Efficiency: ReLu is faster to compute than the sigmoid function, and its derivative is faster to compute. This makes a significant difference to training and inference time for neural networks: only a constant factor, but constants can matter.\n",
    "\n",
    "- Simplicity: ReLu is simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cefeb37-9bf5-4a92-bb49-5a203a2d6f7d",
   "metadata": {},
   "source": [
    "#### Q7. Explain the concept of \"leaky ReLU\" and how it addresses the vanishing gradient problem.\n",
    "- leaky ReLU to maps the values less than zeros to a very small positive number. This prevents vanishing gradient from occurring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79760c68-da12-4aee-ad30-a01da26f5365",
   "metadata": {},
   "source": [
    "#### Q8. What is the purpose of the softmax activation function? When is it commonly used?\n",
    "- The softmax function is used as the activation function in the output layer of neural network models that predict a multinomial probability distribution. That is, softmax is used as the activation function for multi-class classification problems where class membership is required on more than two class labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd8834f-abe4-45ac-ad5a-ec6b35c381c5",
   "metadata": {},
   "source": [
    "#### Q9. What is the hyperbolic tangent (tanh) activation function? How does it compare to the sigmoid function?\n",
    "- The tanh function takes any real value as input and outputs values in the range -1 to 1.\n",
    "- sigmoid lies between 0 and 1. whereas tanh lies between 1 and -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340ae461-22fa-403e-a1ff-c6dcabf5d55d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
