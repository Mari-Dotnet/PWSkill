{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd3d35f9-a370-4e51-97b4-64b058cb1604",
   "metadata": {},
   "source": [
    "#### Which activation function is commonly used for binary classification tasks?\n",
    "- Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2826ab90-2532-4cdc-a3ff-56369d7a1764",
   "metadata": {},
   "source": [
    "#### Which activation function is suitable for handling the vanishing gradient problem in deep neural networks?\n",
    "- Leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b9731b-44ff-4d37-aca6-4477d7d11845",
   "metadata": {},
   "source": [
    "#### Which activation function is commonly used for multi-class classification tasks?\n",
    "- Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a51d61-d9a3-480c-9026-eb12fd841baa",
   "metadata": {},
   "source": [
    "#### Which activation function is preferred for most hidden layers in a deep neural network?\n",
    "- ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a1a9bc-aafb-4829-bcb0-ce758bcc5402",
   "metadata": {},
   "source": [
    "#### Which activation function can produce negative values and is centered around zero?\n",
    "- Tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903d95f9-e3a2-4fad-8009-6c1871ba1c45",
   "metadata": {},
   "source": [
    "#### Which activation function is a variant of ReLU that allows a small gradient for negative values?\n",
    "- Leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7e4c62-92fa-4eb3-9bd8-83786b706a45",
   "metadata": {},
   "source": [
    "#### Which activation function can map any real-valued number to a value between 0 and 1?\n",
    "- Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ddf54e-9067-4c1b-bfe7-f669df20f9ab",
   "metadata": {},
   "source": [
    "#### Which activation function does not introduce non-linearity to the model?\n",
    "- Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7c6ae9-65fa-4751-8f66-d2996c5d6265",
   "metadata": {},
   "source": [
    "#### Which activation function is used for binary classification tasks where the output ranges from -1 to 1?\n",
    "- Tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d10881e-1a9e-44f7-9114-d2f14810a905",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Which activation function is less prone to the \"vanishing gradient\" problem compared to the sigmoid function?\n",
    "- ReLU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
