{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6af6486-20e2-4149-9891-1f9426e6a4dd",
   "metadata": {},
   "source": [
    "What is the benefit of using bootstrapped samples in Bagging?\n",
    "- It provides diversity in the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456b52a1-6756-4132-9f58-d953ca2055ae",
   "metadata": {},
   "source": [
    "What are ensemble techniques in machine learning?\n",
    "- Techniques used to create an ensemble of multiple models and combine their predictions to improve overall performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246f4a52-1f6c-4d69-aee9-7c1765eae17a",
   "metadata": {},
   "source": [
    "in Bagging, how are the predictions of multiple models combined?\n",
    "- By taking the majority vote of the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c1823c-25b5-4d63-9817-5f21b1952884",
   "metadata": {},
   "source": [
    "Which of the following is NOT a characteristic of Bagging?\n",
    "- Using the entire dataset for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e184870-843c-400e-a809-9206b759c910",
   "metadata": {},
   "source": [
    "Which of the following is an advantage of ensemble techniques?\n",
    "- Bagging uses bootstrapped samples from the original dataset for training, not the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e0a066-f27e-4ec1-a96d-794cdf06f341",
   "metadata": {},
   "source": [
    "Which of the following is an advantage of ensemble techniques?\n",
    "- Improved overall performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa9cd8a-5a6c-4990-ab37-e733eb9c7988",
   "metadata": {},
   "source": [
    " What is the primary purpose of bagging in Random Forest?\n",
    "- The primary purpose of bagging (Bootstrap Aggregating) in Random Forest is to reduce overfitting. Bagging involves training multiple decision trees on bootstrapped samples from the original dataset, which helps to reduce the model's tendency to overfit by averaging the predictions of the individual trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f41b40-d4cd-4f00-81d9-b56ee54fd3a8",
   "metadata": {},
   "source": [
    "What is the main advantage of using Random Forest over a single decision tree?\n",
    "- Random Forest is less prone to overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc7f102-bc76-4b35-9a48-b8f6c10b2f34",
   "metadata": {},
   "source": [
    "How are feature subsets selected in Random Forest?\n",
    "- Random Forest selects a random subset of features for each tree in the ensemble. This helps to introduce diversity in the individual trees and reduce the chance of overfitting, as each tree is trained on a different set of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd6c3ab-c4e2-4866-8b76-2d417b3c066f",
   "metadata": {},
   "source": [
    "What is the purpose of using bootstrapped samples in Random Forest?\n",
    "- To introduce diversity among the trees. \n",
    "- Bootstrapped samples are used in Random Forest to introduce diversity among the trees in the ensemble. Each tree is trained on a random subset of samples with replacement from the original dataset, which helps to reduce the chance of overfitting and improve the model's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31bd974-fe8c-450d-8e04-61614fc93509",
   "metadata": {},
   "source": [
    "What is the criterion used for splitting nodes in a Random Forest?\n",
    "- Random Forest can use either Gini impurity or information gain as the criterion for splitting nodes in the decision trees. Both are common measures used to evaluate the impurity or purity of a node in a decision tree and help to make optimal splits during tree construction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48d141e-7141-4860-bdaf-572c0fa1efa2",
   "metadata": {},
   "source": [
    "#### What is the main idea behind the Random Forest Regressor?\n",
    "- Combining multiple weak models to create a stronger model\n",
    "- The main idea behind the Random Forest Regressor is to combine the predictions of multiple weak models, typically decision trees, to create a stronger and more accurate model. This process is known as ensemble learning, where the weak models are combined to reduce bias and variance and improve the overall performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb94f9c-0d74-4b82-86ec-397868f3a883",
   "metadata": {},
   "source": [
    "#### What is the criterion used for splitting nodes in a Random Forest Regressor?\n",
    "- Mean squared error (MSE) \n",
    "- In Random Forest Regressor, the criterion used for splitting nodes in decision trees is typically the mean squared error (MSE). MSE measures the average squared difference between the predicted and actual values of the target variable. The node that results in the minimum MSE after splitting is selected as the splitting point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0e9458-016e-4752-b0b1-ffb0801e39cb",
   "metadata": {},
   "source": [
    "#### How are the decision trees combined in a Random Forest Regressor?\n",
    "- By taking the average of their predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f0376d-6829-468c-96b5-cd782f3e474d",
   "metadata": {},
   "source": [
    "#### What is the purpose of random feature selection in a Random Forest Regressor?\n",
    "- Random feature selection is a technique used in Random Forest Regressor to avoid overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494c157c-ddeb-430e-9352-74bea1cae5eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### What is the default number of trees in a Random Forest Regressor in scikit-learn library in Python?\n",
    "- 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd657d7-4efb-4349-94f9-5fcc67b369e6",
   "metadata": {},
   "source": [
    "####  What is the purpose of bagging in Random Forest Classifier?\n",
    "- Bagging (Bootstrap Aggregating) is a technique used in Random Forest Classifier where multiple decision trees are trained on randomly sampled subsets of features and data samples with replacement. This helps in reducing the correlation between trees and improves the diversity and accuracy of the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb4dcd6-b689-4753-8d06-e4be47adb889",
   "metadata": {},
   "source": [
    "#### What is the maximum number of decision trees in a Random Forest Classifier?\n",
    "- A predefined hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c542f22c-81b2-4a92-bb7b-2ea806b890b7",
   "metadata": {},
   "source": [
    "#### What is the purpose of feature importance in Random Forest Classifier?\n",
    "- To rank the importance of different features\n",
    "- Feature importance in Random Forest Classifier is a measure that ranks the importance of different features used in the model. It helps in identifying which features contribute the most towards making accurate predictions and can be useful for feature selection and model interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a985fe-f877-4747-bcb5-afe343a2a42a",
   "metadata": {},
   "source": [
    "#### How does Random Forest Classifier handle missing values?\n",
    "-  Random Forest Classifier handles missing values by using surrogate splits. Surrogate splits are additional splits that are used in decision trees when a feature has a missing value for a data sample. These surrogate splits help in determining the best path for the sample down the tree, even if the original feature has a missing value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568a0c1a-c067-484a-aa75-cf4cb26e909d",
   "metadata": {},
   "source": [
    "#### What is the main advantage of using a Random Forest Classifier over a single decision tree?\n",
    "- Higher accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bc153f-6320-45d0-a40b-2d71c6549dc9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### What is the purpose of out-of-bag (OOB) samples in Random Forest Classifier?\n",
    "- To validate the model during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83e5c18-cb74-4b62-9e21-d31e4c2da6cb",
   "metadata": {},
   "source": [
    "#### What is the purpose of using a pipeline in Random Forest classifier?\n",
    "- To automate the feature engineering process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fd90a8-97f7-44a8-a590-eba8527682c8",
   "metadata": {},
   "source": [
    "#### What is hyperparameter tuning in the context of Random Forest classifier?\n",
    "- Automatically tuning the hyperparameters of the Random Forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833890dd-44a8-4398-86f1-a00ef8492f60",
   "metadata": {},
   "source": [
    "#### What is the main benefit of using feature engineering automation in Random Forest classifier?\n",
    "- Better model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a54a080-c58d-4593-99bf-7e91b583d569",
   "metadata": {},
   "source": [
    "#### What is the purpose of using hyperparameter tuning in Random Forest classifier?\n",
    "- To find the best values for  hyperparameters of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbe4851-473e-4449-85d9-67c7329b9db2",
   "metadata": {},
   "source": [
    "#### What is the purpose of using a pipeline in Random Forest classifier with feature engineering automation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0bf548-a2d8-4c42-908b-6afd82c4debb",
   "metadata": {},
   "source": [
    "####"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
