{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6af6486-20e2-4149-9891-1f9426e6a4dd",
   "metadata": {},
   "source": [
    "What is the benefit of using bootstrapped samples in Bagging?\n",
    "- It provides diversity in the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456b52a1-6756-4132-9f58-d953ca2055ae",
   "metadata": {},
   "source": [
    "What are ensemble techniques in machine learning?\n",
    "- Techniques used to create an ensemble of multiple models and combine their predictions to improve overall performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246f4a52-1f6c-4d69-aee9-7c1765eae17a",
   "metadata": {},
   "source": [
    "in Bagging, how are the predictions of multiple models combined?\n",
    "- By taking the majority vote of the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c1823c-25b5-4d63-9817-5f21b1952884",
   "metadata": {},
   "source": [
    "Which of the following is NOT a characteristic of Bagging?\n",
    "- Using the entire dataset for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e184870-843c-400e-a809-9206b759c910",
   "metadata": {},
   "source": [
    "Which of the following is an advantage of ensemble techniques?\n",
    "- Bagging uses bootstrapped samples from the original dataset for training, not the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e0a066-f27e-4ec1-a96d-794cdf06f341",
   "metadata": {},
   "source": [
    "Which of the following is an advantage of ensemble techniques?\n",
    "- Improved overall performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa9cd8a-5a6c-4990-ab37-e733eb9c7988",
   "metadata": {},
   "source": [
    " What is the primary purpose of bagging in Random Forest?\n",
    "- The primary purpose of bagging (Bootstrap Aggregating) in Random Forest is to reduce overfitting. Bagging involves training multiple decision trees on bootstrapped samples from the original dataset, which helps to reduce the model's tendency to overfit by averaging the predictions of the individual trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f41b40-d4cd-4f00-81d9-b56ee54fd3a8",
   "metadata": {},
   "source": [
    "What is the main advantage of using Random Forest over a single decision tree?\n",
    "- Random Forest is less prone to overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc7f102-bc76-4b35-9a48-b8f6c10b2f34",
   "metadata": {},
   "source": [
    "How are feature subsets selected in Random Forest?\n",
    "- Random Forest selects a random subset of features for each tree in the ensemble. This helps to introduce diversity in the individual trees and reduce the chance of overfitting, as each tree is trained on a different set of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd6c3ab-c4e2-4866-8b76-2d417b3c066f",
   "metadata": {},
   "source": [
    "What is the purpose of using bootstrapped samples in Random Forest?\n",
    "- To introduce diversity among the trees. \n",
    "- Bootstrapped samples are used in Random Forest to introduce diversity among the trees in the ensemble. Each tree is trained on a random subset of samples with replacement from the original dataset, which helps to reduce the chance of overfitting and improve the model's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31bd974-fe8c-450d-8e04-61614fc93509",
   "metadata": {},
   "source": [
    "What is the criterion used for splitting nodes in a Random Forest?\n",
    "- Random Forest can use either Gini impurity or information gain as the criterion for splitting nodes in the decision trees. Both are common measures used to evaluate the impurity or purity of a node in a decision tree and help to make optimal splits during tree construction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48d141e-7141-4860-bdaf-572c0fa1efa2",
   "metadata": {},
   "source": [
    "#### What is the main idea behind the Random Forest Regressor?\n",
    "- Combining multiple weak models to create a stronger model\n",
    "- The main idea behind the Random Forest Regressor is to combine the predictions of multiple weak models, typically decision trees, to create a stronger and more accurate model. This process is known as ensemble learning, where the weak models are combined to reduce bias and variance and improve the overall performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb94f9c-0d74-4b82-86ec-397868f3a883",
   "metadata": {},
   "source": [
    "#### What is the criterion used for splitting nodes in a Random Forest Regressor?\n",
    "- Mean squared error (MSE) \n",
    "- In Random Forest Regressor, the criterion used for splitting nodes in decision trees is typically the mean squared error (MSE). MSE measures the average squared difference between the predicted and actual values of the target variable. The node that results in the minimum MSE after splitting is selected as the splitting point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0e9458-016e-4752-b0b1-ffb0801e39cb",
   "metadata": {},
   "source": [
    "#### How are the decision trees combined in a Random Forest Regressor?\n",
    "- By taking the average of their predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f0376d-6829-468c-96b5-cd782f3e474d",
   "metadata": {},
   "source": [
    "#### What is the purpose of random feature selection in a Random Forest Regressor?\n",
    "- Random feature selection is a technique used in Random Forest Regressor to avoid overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494c157c-ddeb-430e-9352-74bea1cae5eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### What is the default number of trees in a Random Forest Regressor in scikit-learn library in Python?\n",
    "- 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3394537a-b895-4817-a00f-5c50322c3d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
