{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d35be484-9852-4929-b690-3d7e523a0a62",
   "metadata": {},
   "source": [
    "#### What is the main goal of Boosting Technique in machine learning?\n",
    "- The main objective of Boosting Technique is to combine multiple weak learners (models) to create a strong learner that improves the accuracy of the model by reducing bias and variance errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2c227c-b171-4ec2-9596-4e8a5a271fed",
   "metadata": {},
   "source": [
    "#### Which algorithm is used in AdaBoost for adjusting the weights of misclassified samples?\n",
    "- AdaBoost adjusts the weights of misclassified samples using a weighted approach, where misclassified samples are given higher weights to emphasize their importance in subsequent iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff6d0a2-5455-4b28-96a8-4a3daf00fbbd",
   "metadata": {},
   "source": [
    "#### What is the key idea behind Gradient Boosting?\n",
    "- Combining the predictions of weak learners in a weighted manner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b78993d-942c-4833-aafb-1e99a120ece5",
   "metadata": {},
   "source": [
    "#### What does \"Ada\" stand for in AdaBoost?\n",
    "-  \"Ada\" in AdaBoost stands for Adaptive, as the algorithm adaptively adjusts the weights of misclassified samples to improve the accuracy of the model in subsequent iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c9c0b4-0bcb-4d0e-826f-01d61bacc2bb",
   "metadata": {},
   "source": [
    "#### What type of learning does AdaBoost belong to?\n",
    "-  AdaBoost is a supervised learning algorithm as it uses labeled data to train a model and make predictions on new, unseen data based on the patterns learned from the labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6867a31c-c011-4750-9991-486f9082632c",
   "metadata": {},
   "source": [
    "#### Which of the following is a weak learner used in AdaBoost?\n",
    "-  A decision stump is a simple decision tree with only one level, making it a weak learner. AdaBoost combines multiple decision stumps to create a strong learner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1cf3b9-abf4-42a0-bf75-19a576364bcc",
   "metadata": {},
   "source": [
    "#### What is the key concept behind Gradient Boosting?\n",
    "- Ensemble learning\n",
    "- Gradient Boosting is an ensemble learning method, which means it combines the predictions of multiple weak models (usually decision trees) to create a stronger model. By sequentially building decision trees that correct the errors of previous trees, Gradient Boosting creates a powerful ensemble model that can make accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dfefcb-0210-47f1-aa88-7aa1f10493c2",
   "metadata": {},
   "source": [
    "#### What is the purpose of regularization in Gradient Boosting?\n",
    "- Regularization techniques such as L1 (Lasso) and L2 (Ridge) regularization are used in Gradient Boosting to prevent overfitting. Overfitting occurs when the model learns to memorize the training data instead of generalizing from it, resulting in poor performance on unseen data. Regularization helps to constrain the model and prevent overfitting, leading to a more robust and accurate mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d99e4f-23a6-4421-9115-664af7c0542f",
   "metadata": {},
   "source": [
    "#### What is the role of learning rate in Gradient Boosting?\n",
    "- It scales the contribution of each tree to the ensemble\n",
    "- A smaller learning rate reduces the impact of each tree, making the model more conservative, while a larger learning rate gives more weight to each tree, making the model more aggressive. Finding an appropriate learning rate is important as it affects the trade-off between bias and variance in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f84650a-e96a-4984-8f8c-022dcf400291",
   "metadata": {},
   "source": [
    "#### What is the main advantage of Gradient Boosting over other machine learning algorithms?\n",
    "- Gradient Boosting is known for its high accuracy and predictive power. By building an ensemble of weak models that correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8672510b-8b85-4187-9a17-b1fe062aaa7d",
   "metadata": {},
   "source": [
    "#### What is the effect of increasing the number of boosting iterations in Gradient Boosting?\n",
    "- Increases the model complexity\n",
    "- Increasing the number of boosting iterations in Gradient Boosting leads to a more complex model. Each iteration adds a new decision tree to the ensemble, and the combined effect of multiple trees can result in a more complex and powerful model. However, increasing the number of boosting iterations also increases the risk of overfitting, so it should be carefully tuned to find the optimal balance between complexity and generalization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91f539c4",
   "metadata": {},
   "source": [
    "#### Which of the following is not a hyperparameter in Xgboost?\n",
    "- Hyperparameters are the parameters that are not learned by the model, but are set by the user before training the model. Xgboost hyperparameters include the number of trees, learning rate, max depth, and others, but the number of features is not a hyperparameter in Xgboost."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba7b7f72",
   "metadata": {},
   "source": [
    "#### Which of the following is a common technique to prevent overfitting in Xgboost?\n",
    "- Using regularization\n",
    "- Overfitting occurs when a model is too complex and fits the training data too closely, resulting in poor performance on unseen data. Regularization is a technique that adds a penalty to the loss function to discourage the model from overfitting the training data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cec60b80",
   "metadata": {},
   "source": [
    "#### What is the default objective function in Xgboost for binary classification problems?\n",
    "- Binary cross-entropy\n",
    "- The objective function is the function that is minimized during training to optimize the model's performance. For binary classification problems, binary cross-entropy is commonly used as the objective function, and this is the default in Xgboost."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44c249c0",
   "metadata": {},
   "source": [
    "#### Which of the following is a limitation of Xgboost?\n",
    "- Xgboost is a powerful machine learning algorithm that is widely used in industry. However, one limitation of Xgboost is that it can require large amounts of training data to achieve good performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff70751b",
   "metadata": {},
   "source": [
    "#### Which of the following is a parameter in Xgboost that controls the trade-off between overfitting and underfitting?\n",
    "- Gamma is a regularization parameter in Xgboost that controls the minimum reduction in the loss required to make a further partition on a leaf node of the tree. A higher value of gamma leads to a more conservative model that avoids overfitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a71a273f",
   "metadata": {},
   "source": [
    "#### Which of the following is a parameter in Xgboost that controls the fraction of observations to be randomly sampled for each tree?\n",
    "- Subsample is a technique to introduce randomness in the training process and avoid overfitting. It controls the fraction of observations to be randomly sampled for each tree. A value of 1.0 means that all observations are used for each tree, while a value less than 1.0 means that a fraction of the observations is used."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d178454",
   "metadata": {},
   "source": [
    "#### Which of the following is a parameter in Xgboost that controls the weights of positive and negative examples in the loss function?\n",
    "- Scale positive weight is a hyperparameter in Xgboost that controls the weights of positive and negative examples in the loss function for binary classification problems. This parameter can be used to balance the classes when the data is imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ea09f2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
