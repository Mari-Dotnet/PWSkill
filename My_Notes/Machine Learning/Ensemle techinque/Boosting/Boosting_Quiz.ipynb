{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d35be484-9852-4929-b690-3d7e523a0a62",
   "metadata": {},
   "source": [
    "#### What is the main goal of Boosting Technique in machine learning?\n",
    "- The main objective of Boosting Technique is to combine multiple weak learners (models) to create a strong learner that improves the accuracy of the model by reducing bias and variance errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2c227c-b171-4ec2-9596-4e8a5a271fed",
   "metadata": {},
   "source": [
    "#### Which algorithm is used in AdaBoost for adjusting the weights of misclassified samples?\n",
    "- AdaBoost adjusts the weights of misclassified samples using a weighted approach, where misclassified samples are given higher weights to emphasize their importance in subsequent iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff6d0a2-5455-4b28-96a8-4a3daf00fbbd",
   "metadata": {},
   "source": [
    "#### What is the key idea behind Gradient Boosting?\n",
    "- Combining the predictions of weak learners in a weighted manner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b78993d-942c-4833-aafb-1e99a120ece5",
   "metadata": {},
   "source": [
    "#### What does \"Ada\" stand for in AdaBoost?\n",
    "-  \"Ada\" in AdaBoost stands for Adaptive, as the algorithm adaptively adjusts the weights of misclassified samples to improve the accuracy of the model in subsequent iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c9c0b4-0bcb-4d0e-826f-01d61bacc2bb",
   "metadata": {},
   "source": [
    "#### What type of learning does AdaBoost belong to?\n",
    "-  AdaBoost is a supervised learning algorithm as it uses labeled data to train a model and make predictions on new, unseen data based on the patterns learned from the labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6867a31c-c011-4750-9991-486f9082632c",
   "metadata": {},
   "source": [
    "#### Which of the following is a weak learner used in AdaBoost?\n",
    "-  A decision stump is a simple decision tree with only one level, making it a weak learner. AdaBoost combines multiple decision stumps to create a strong learner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1cf3b9-abf4-42a0-bf75-19a576364bcc",
   "metadata": {},
   "source": [
    "#### What is the key concept behind Gradient Boosting?\n",
    "- Ensemble learning\n",
    "- Gradient Boosting is an ensemble learning method, which means it combines the predictions of multiple weak models (usually decision trees) to create a stronger model. By sequentially building decision trees that correct the errors of previous trees, Gradient Boosting creates a powerful ensemble model that can make accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dfefcb-0210-47f1-aa88-7aa1f10493c2",
   "metadata": {},
   "source": [
    "#### What is the purpose of regularization in Gradient Boosting?\n",
    "- Regularization techniques such as L1 (Lasso) and L2 (Ridge) regularization are used in Gradient Boosting to prevent overfitting. Overfitting occurs when the model learns to memorize the training data instead of generalizing from it, resulting in poor performance on unseen data. Regularization helps to constrain the model and prevent overfitting, leading to a more robust and accurate mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d99e4f-23a6-4421-9115-664af7c0542f",
   "metadata": {},
   "source": [
    "#### What is the role of learning rate in Gradient Boosting?\n",
    "- It scales the contribution of each tree to the ensemble\n",
    "- A smaller learning rate reduces the impact of each tree, making the model more conservative, while a larger learning rate gives more weight to each tree, making the model more aggressive. Finding an appropriate learning rate is important as it affects the trade-off between bias and variance in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f84650a-e96a-4984-8f8c-022dcf400291",
   "metadata": {},
   "source": [
    "#### What is the main advantage of Gradient Boosting over other machine learning algorithms?\n",
    "- Gradient Boosting is known for its high accuracy and predictive power. By building an ensemble of weak models that correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8672510b-8b85-4187-9a17-b1fe062aaa7d",
   "metadata": {},
   "source": [
    "#### What is the effect of increasing the number of boosting iterations in Gradient Boosting?\n",
    "- Increases the model complexity\n",
    "- Increasing the number of boosting iterations in Gradient Boosting leads to a more complex model. Each iteration adds a new decision tree to the ensemble, and the combined effect of multiple trees can result in a more complex and powerful model. However, increasing the number of boosting iterations also increases the risk of overfitting, so it should be carefully tuned to find the optimal balance between complexity and generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2176c504-652b-49bd-9f3f-2d2dc30d135b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
