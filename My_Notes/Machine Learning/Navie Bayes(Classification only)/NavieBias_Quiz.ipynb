{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c843c65-0dd7-465f-b501-0857d34dabd1",
   "metadata": {},
   "source": [
    "####  In Multinomial Naive Bayes, what is the likelihood function used to model the conditional probability of the features given the class?\n",
    "\n",
    "- In Multinomial Naive Bayes, the likelihood function used to model the conditional probability of the features given the class is the Multinomial distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8066e8-6ad0-4785-9f4f-9ad8222ab5a6",
   "metadata": {},
   "source": [
    "#### In Multinomial Naive Bayes, how are the probabilities of the features calculated?\n",
    "- In Multinomial Naive Bayes, the probabilities of the features are calculated by assuming that the features are independent, which is why it is called \"naive\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c050dc3-b9b6-439e-81b9-2a0b8e87bc52",
   "metadata": {},
   "source": [
    " #### What is the output of a Gaussian Naive Bayes classifier?\n",
    " - The output of a Gaussian Naive Bayes classifier is a probability distribution over classes. For each instance, the classifier calculates the probability of that instance belonging to each class, based on the values of its features. The class with the highest probability is then chosen as the predicted class for that instance. This output is useful because it not only provides a predicted class label, but also a measure of confidence in that prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27220b66-3320-4c10-8512-90aadb235954",
   "metadata": {},
   "source": [
    "#### Which type of data is Gaussian Naive Bayes best suited for?\n",
    "- Numerical data\n",
    "- Gaussian Naive Bayes is best suited for numerical data, where each feature has a continuous value. This is because Gaussian Naive Bayes assumes that the distribution of each feature is Gaussian (normal), which is a good assumption for numerical data. For categorical data, a different type of Naive Bayes classifier such as Multinomial Naive Bayes would be more appropriate. Text and image data are usually preprocessed into numerical data before being used with Naive Bayes classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a111be-423f-4b1d-b9a1-db87b910f6c9",
   "metadata": {},
   "source": [
    "#### In Bayesian inference, what is the prior distribution?\n",
    "- The distribution of the parameter or hypothesis before seeing the data\n",
    "- : The prior distribution in Bayesian inference is the probability distribution that represents our knowledge or beliefs about the parameter or hypothesis before seeing the data. This prior distribution is updated using Bayes' theorem to obtain the posterior distribution, which represents our knowledge or beliefs about the parameter or hypothesis after seeing the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fdd4d4-a1de-4af7-922c-d0ac50ba586f",
   "metadata": {},
   "source": [
    "#### What is the difference between Bayesian inference and classical (frequentist) inference?\n",
    "- Bayesian inference involves the use of prior knowledge, while classical inference does not.\n",
    "- Bayesian inference involves the use of prior knowledge or beliefs about a parameter or hypothesis, which are updated in light of new data using Bayes' theorem. Classical inference, on the other hand, does not involve the use of prior knowledge and instead relies on the properties of the data sample and the sampling distribution. Bayesian inference is often used in cases where prior knowledge or expertise can provide valuable insights or help to inform the analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73c48b6-365a-4cc1-8c1d-7400b9496ecc",
   "metadata": {},
   "source": [
    "#### What is a confusion matrix used for?\n",
    "-  Evaluating classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588d93a0-9505-4eb6-91f8-e2d725b4d7c4",
   "metadata": {},
   "source": [
    "#### Which of the following metrics measures the proportion of true positives among all positive predictions?\n",
    "- Precision measures the proportion of true positives among all positive predictions made by the model. It is calculated as TP / (TP + FP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d38c0ee-8719-4d9f-882a-d213217531ee",
   "metadata": {},
   "source": [
    "#### Which of the following metrics measures the proportion of true positives among all actual positives?\n",
    "- Recall measures the proportion of true positives among all actual positive instances in the dataset. It is calculated as TP / (TP + FN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e21cba-334f-4978-a88f-78a89a074a54",
   "metadata": {},
   "source": [
    "####  What is a ROC curve used for?\n",
    "- An ROC (Receiver Operating Characteristic) curve is used to evaluate the performance of a binary classification model. It plots the true positive rate (TPR) against the false positive rate (FPR) for different classification thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d42ce0c-6869-450c-8969-d0aebefc1e0b",
   "metadata": {},
   "source": [
    "#### Which of the following is a measure of how well a binary classifier can distinguish between positive and negative examples?\n",
    "- AUC (Area Under the Curve) is a measure of how well a binary classifier can distinguish between positive and negative examples. It measures the area under the ROC curve and provides a single score to evaluate the overall performance of a classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c91acd-977a-4c53-ba6f-024b15fc4ea1",
   "metadata": {},
   "source": [
    "#### Which of the following is true for a perfect classifier?\n",
    "\n",
    "- Precision = Recall = F1 score = 1.0\n",
    "- A perfect classifier correctly predicts all positive and negative instances in the dataset, resulting in precision, recall, and F1 score of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8486676f-6181-4410-94c8-bc7ececdb0da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
