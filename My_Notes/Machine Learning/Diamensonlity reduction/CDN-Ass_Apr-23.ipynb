{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbae17d9-5eb0-4d45-83ec-265839a95bfb",
   "metadata": {},
   "source": [
    "#### Q1. What is the curse of dimensionality reduction and why is it important in machine learning?\n",
    "- To reduce the more number of feature in to less number of feature and get good performacne of the model.\n",
    "- prevent the curcse of diamensloity\n",
    "- improve the performace of the model\n",
    "- visualize the data in 3d and 2d to understand the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fbbb0c-83e8-4915-8037-31be2fa31485",
   "metadata": {},
   "source": [
    "#### Q2. How does the curse of dimensionality impact the performance of machine learning algorithms?\n",
    "- yes it will impact the machine learning algorithm, if number of feature is more so that model giving poor performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d34240-6c45-4978-a34a-7cd306c0cafc",
   "metadata": {},
   "source": [
    "#### Q3. What are some of the consequences of the curse of dimensionality in machine learning, and how do they impact model performance?\n",
    "- Feature selection\n",
    "    - take only importand feature\n",
    "- Dimensolity reduction (PCA)\n",
    "    - feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1faa6a-8ca9-45c5-b99a-3103943e4088",
   "metadata": {},
   "source": [
    "#### Q4. Can you explain the concept of feature selection and how it can help with dimensionality reduction?\n",
    "- co-variance\n",
    "    -  in feature selection we can get co-varinace of input and output if values is positive then it's related if its zero there is no relatioshship between input and output feature so we can remove that feature.\n",
    "- co-relation\n",
    "    - pearson co-relation values is 0 to 1\n",
    "    - the values is towards to value of 1 its highly corealted\n",
    "    -  the values is towards to value of -1 its not  corealted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9280e6d-0f6c-4d82-ab9e-48a64c97072a",
   "metadata": {},
   "source": [
    "#### Q5. What are some limitations and drawbacks of using dimensionality reduction techniques in machine learning?\n",
    "- One limitation of PCA is that it may not always capture the most important variations in the data,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a15d978-8027-4dd5-befa-fa97ea948faa",
   "metadata": {},
   "source": [
    "#### Q6. How does the curse of dimensionality relate to overfitting and underfitting in machine learning?\n",
    "- Due to the curse of dimensionality, if the number of attributes increases, the algorithm will tend to overfit and feature selection, for obvious reasons, will try to decrease the number of attributes, which means less overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29838201-5870-4952-aab5-42069eb722d7",
   "metadata": {},
   "source": [
    "#### Q7. How can one determine the optimal number of dimensions to reduce data to when using dimensionality reduction techniques?\n",
    "- A Scree plot is a graphical representation of the eigenvalues of the principal components in a PCA analysis. It is used to determine the optimal number of principal components to retain in the dataset, based on the magnitude of the eigenvalues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b81e70-d3c5-4006-8dba-4a19dbd00eee",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
