{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e62a126a-d4f3-4728-b9b3-54c94370c703",
   "metadata": {},
   "source": [
    "#### What is the Curse of Dimensionality?\n",
    "- The Curse of Dimensionality is a phenomenon in which the performance of machine learning algorithms deteriorates as the number of features or dimensions increases. This occurs because the number of training examples required to accurately represent the space grows exponentially with the number of dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cde56ad-bad4-4477-884a-be40b59ff126",
   "metadata": {},
   "source": [
    "#### How does dimensionality reduction help with the Curse of Dimensionality?\n",
    "- By reducing the number of features or dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16c5d9a-882a-4417-a9ce-a8c52384508f",
   "metadata": {},
   "source": [
    "#### Which of the following is a disadvantage of using dimensionality reduction techniques?\n",
    "- \n",
    "They can introduce noise and lose information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635bdd28-12dd-4725-a16f-a14ea52c6b55",
   "metadata": {},
   "source": [
    "#### Which of the following is a type of dimensionality reduction technique that preserves the pairwise distances between data points?\n",
    "- t-SNE is a type of dimensionality reduction technique that preserves the pairwise distances between data points. It is commonly used for visualizing high-dimensional data in two or three dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fa3f54-f23a-4b98-9aff-2d8b71dbe95f",
   "metadata": {},
   "source": [
    "#### Which of the following statements is true about the Curse of Dimensionality?\n",
    "- It makes it harder to find meaningful patterns in high-dimensional data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12002e9b-7ca9-49e7-a00b-8fdf16e06c03",
   "metadata": {},
   "source": [
    "#### Which of the following is a dimensionality reduction technique that seeks to preserve the local structure of the data?\n",
    "- Isomap is a dimensionality reduction technique that seeks to preserve the local structure of the data. It creates a low-dimensional representation of the data that preserves the pairwise geodesic distances between data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9ef0b8-b6f5-4773-b0ef-dc635b1aed54",
   "metadata": {},
   "source": [
    "#### What is the mathematical technique used in PCA?\n",
    "- PCA is based on the mathematical technique of Singular Value Decomposition (SVD), which is used to decompose a matrix into its constituent parts. In the case of PCA, SVD is used to decompose the covariance matrix of a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be69755-2ded-4691-9a0d-6540a2c840a5",
   "metadata": {},
   "source": [
    "#### What is a Scree plot used for in PCA?\n",
    "- A Scree plot is a graphical representation of the eigenvalues of the principal components in a PCA analysis. It is used to determine the optimal number of principal components to retain in the dataset, based on the magnitude of the eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466c696a-a4f5-4464-b3bf-9838efb61f1b",
   "metadata": {},
   "source": [
    "#### Which of the following statements is true about PCA?\n",
    "- PCA is used to reduce the dimensionality of a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf8990f-d89b-4e29-9fc5-3ef45ebce72e",
   "metadata": {},
   "source": [
    "#### What is the role of eigenvalues in PCA?\n",
    "- Eigenvalues are used to determine the optimal number of principal components to retain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadcfbbf-6bf0-46e5-9207-b18de18034f3",
   "metadata": {},
   "source": [
    "#### How is PCA useful in machine learning?\n",
    "- It helps in identifying the most important features for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69213f6-dd3c-4648-afc9-1c629a21442c",
   "metadata": {},
   "source": [
    "#### Which of the following is a limitation of PCA?\n",
    "- One limitation of PCA is that it may not always capture the most important variations in the data, especially if the data has complex relationships between the features. Additionally, PCA is a dimensionality reduction technique that can be applied to both supervised and unsupervised learning tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e9abbd-cb34-4511-b57d-d5aa5eecbb92",
   "metadata": {},
   "source": [
    "What is the Eigen-Decomposition approach?\n",
    "- A method for diagonalizing a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84b38c4-63f0-44ce-ab81-837c91cdb0c9",
   "metadata": {},
   "source": [
    "Which of the following conditions must be satisfied for a square matrix to be diagonalizable using the Eigen-Decomposition approach?\n",
    "-  For a square matrix to be diagonalizable using the Eigen-Decomposition approach, it must have a complete set of linearly independent eigenvectors, which requires the matrix to have distinct eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7dc841-95f6-4480-b602-4089ca6be3a9",
   "metadata": {},
   "source": [
    "\n",
    "What is the significance of the spectral theorem in the context of the Eigen-Decomposition approach?\n",
    "- The spectral theorem states that any real symmetric matrix can be diagonalized by an orthogonal matrix, and any complex Hermitian matrix can be diagonalized by a unitary matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a539c0ca-635d-4ce2-9b61-52dc10313e8c",
   "metadata": {},
   "source": [
    " How can the Eigen-Decomposition approach be extended to handle non-square matrices?\n",
    " - Singular value decomposition (SVD) can be used to extend the Eigen-Decomposition approach to handle non-square matrices, by decomposing the matrix into three matrices: U, Σ, and V, where U and V are unitary matrices and Σ is a diagonal matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d67141-4341-47eb-a12a-315b95d3b893",
   "metadata": {},
   "source": [
    "Eigenvectors associated with distinct eigenvalues of a symmetric or Hermitian matrix are always orthogonal to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f3c288-f583-4eec-98cb-5fa08362c839",
   "metadata": {},
   "source": [
    "What is the purpose of the orthogonal matrix in the Eigen-Decomposition approach?\n",
    "- An orthogonal matrix is used to diagonalize a symmetric or Hermitian matrix, and to ensure that the eigenvectors associated with distinct eigenvalues are orthogonal to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bff16d5-ba63-45b2-bd9b-7cf4eefa2767",
   "metadata": {},
   "source": [
    "Which of the following methods can be used to find the eigenvalues of a matrix?\n",
    "- The power iteration method is an iterative algorithm that can be used to find the eigenvalues of a matrix, by computing successive powers of the matrix multiplied by a random vector and normalizing the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15491013-f7eb-44dd-9d76-2996332a8c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
