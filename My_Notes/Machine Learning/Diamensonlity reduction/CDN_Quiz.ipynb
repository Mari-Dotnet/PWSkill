{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e62a126a-d4f3-4728-b9b3-54c94370c703",
   "metadata": {},
   "source": [
    "#### What is the Curse of Dimensionality?\n",
    "- The Curse of Dimensionality is a phenomenon in which the performance of machine learning algorithms deteriorates as the number of features or dimensions increases. This occurs because the number of training examples required to accurately represent the space grows exponentially with the number of dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cde56ad-bad4-4477-884a-be40b59ff126",
   "metadata": {},
   "source": [
    "#### How does dimensionality reduction help with the Curse of Dimensionality?\n",
    "- By reducing the number of features or dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16c5d9a-882a-4417-a9ce-a8c52384508f",
   "metadata": {},
   "source": [
    "#### Which of the following is a disadvantage of using dimensionality reduction techniques?\n",
    "- \n",
    "They can introduce noise and lose information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635bdd28-12dd-4725-a16f-a14ea52c6b55",
   "metadata": {},
   "source": [
    "#### Which of the following is a type of dimensionality reduction technique that preserves the pairwise distances between data points?\n",
    "- t-SNE is a type of dimensionality reduction technique that preserves the pairwise distances between data points. It is commonly used for visualizing high-dimensional data in two or three dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fa3f54-f23a-4b98-9aff-2d8b71dbe95f",
   "metadata": {},
   "source": [
    "#### Which of the following statements is true about the Curse of Dimensionality?\n",
    "- It makes it harder to find meaningful patterns in high-dimensional data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12002e9b-7ca9-49e7-a00b-8fdf16e06c03",
   "metadata": {},
   "source": [
    "#### Which of the following is a dimensionality reduction technique that seeks to preserve the local structure of the data?\n",
    "- Isomap is a dimensionality reduction technique that seeks to preserve the local structure of the data. It creates a low-dimensional representation of the data that preserves the pairwise geodesic distances between data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9ef0b8-b6f5-4773-b0ef-dc635b1aed54",
   "metadata": {},
   "source": [
    "#### What is the mathematical technique used in PCA?\n",
    "- PCA is based on the mathematical technique of Singular Value Decomposition (SVD), which is used to decompose a matrix into its constituent parts. In the case of PCA, SVD is used to decompose the covariance matrix of a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be69755-2ded-4691-9a0d-6540a2c840a5",
   "metadata": {},
   "source": [
    "#### What is a Scree plot used for in PCA?\n",
    "- A Scree plot is a graphical representation of the eigenvalues of the principal components in a PCA analysis. It is used to determine the optimal number of principal components to retain in the dataset, based on the magnitude of the eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466c696a-a4f5-4464-b3bf-9838efb61f1b",
   "metadata": {},
   "source": [
    "#### Which of the following statements is true about PCA?\n",
    "- PCA is used to reduce the dimensionality of a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf8990f-d89b-4e29-9fc5-3ef45ebce72e",
   "metadata": {},
   "source": [
    "#### What is the role of eigenvalues in PCA?\n",
    "- Eigenvalues are used to determine the optimal number of principal components to retain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadcfbbf-6bf0-46e5-9207-b18de18034f3",
   "metadata": {},
   "source": [
    "#### How is PCA useful in machine learning?\n",
    "- It helps in identifying the most important features for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69213f6-dd3c-4648-afc9-1c629a21442c",
   "metadata": {},
   "source": [
    "#### Which of the following is a limitation of PCA?\n",
    "- One limitation of PCA is that it may not always capture the most important variations in the data, especially if the data has complex relationships between the features. Additionally, PCA is a dimensionality reduction technique that can be applied to both supervised and unsupervised learning tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7760ddf-4350-4342-8f40-a65a678a9939",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
