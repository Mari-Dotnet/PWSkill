{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da4f7e4c-eaea-4c5b-809f-8c9579cbc719",
   "metadata": {},
   "source": [
    "### Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?\n",
    "- Ridge Regression adds a penalty term to the cost function that shrinks the coefficients towards zero, reducing the impact of multicollinearity.\n",
    "-  OLS regression does not include any regularization techniques. The purpose of the penalty term is to reduce the impact of multicollinearity in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0485b434-750d-4996-84de-6c59ecc8cc6f",
   "metadata": {},
   "source": [
    "### Q2. What are the assumptions of Ridge Regression?\n",
    "- Linearity: The relationship between the dependent variable and the independent variables should be linear.\n",
    "\n",
    "- Independence of errors: The errors should be independent of each other and not correlated.\n",
    "- Normality: The errors should be normally distributed around zero.\n",
    "- No multicollinearity: The independent variables should not be highly correlated with each other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca396e0-8ef6-4ca8-8f08-c25d7fa6b1b5",
   "metadata": {},
   "source": [
    "### Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?\n",
    "- The value of 位 in Ridge Regression can be selected through cross-validation. The optimal value of 位 is chosen based on the performance metric, such as MSE or R-squared, on a validation set. After selecting the optimal value of 位, the model is evaluated on a separate test set to ensure that it generalizes well to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cecd89-7c9e-4b81-a858-85d4e4325960",
   "metadata": {},
   "source": [
    "### Q5. How does the Ridge Regression model perform in the presence of multicollinearity?\n",
    "- Ridge Regression is a regularized linear regression model that adds an L2 penalty term to the loss function. The L2 penalty term adds a penalty proportional to the square of the magnitude of the coefficients. This penalty shrinks the coefficients towards zero, which helps to reduce overfitting and improve the generalization performance of the model. Ridge Regression is particularly useful when dealing with multicollinearity (high correlation) among the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08971de9-0eb6-459b-8997-7f755987132b",
   "metadata": {},
   "source": [
    "### Q4. Can Ridge Regression be used for feature selection? If yes, how?\n",
    "- Ridge regression not used for feature selection it only used for to reduce the overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddc66f5-25f0-4495-851d-0dd050d9a396",
   "metadata": {},
   "source": [
    "#### Q6. Can Ridge Regression handle both categorical and continuous independent variables?\n",
    "- Yes, Ridge Regression can handle both categorical and continuous independent variables. In Ridge Regression, the independent variables can be continuous, categorical or a combination of both.\n",
    "- Categorical variables need to be encoded into a numerical format before fitting the Ridge Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7eed79-c950-485e-aefd-42d786e9ab37",
   "metadata": {},
   "source": [
    "### Q7. How do you interpret the coefficients of Ridge Regression?\n",
    "- The coefficients in Ridge Regression are subject to shrinkage, which means that they are reduced towards zero to prevent overfitting. The amount of shrinkage applied to the coefficients is controlled by the regularization parameter, 位."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f016a4-48ae-4a60-8aef-157a134f0657",
   "metadata": {},
   "source": [
    "### Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?\n",
    "- Ridge Regression can be used for time-series data analysis, but it requires some modifications to account for the temporal dependencies in the data. The traditional Ridge Regression assumes that the observations are independent of each other, which is not true for time-series data, where each observation is dependent on the previous observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26c4113-0918-46c9-9d9e-48be36328f37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
