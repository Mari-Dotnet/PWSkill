{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84dd1188-6c5b-45f3-881f-5716f39d0982",
   "metadata": {},
   "source": [
    "#### Q1. What is the KNN algorithm?\n",
    "- K nearest neighbhout to take the nearest points and calculate the predicetd output."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60519f8c-ef4e-4b7f-9928-5a9318f0cecb",
   "metadata": {},
   "source": [
    "#### Q2. How do you choose the value of K in KNN?\n",
    "- The value of k in KNN is a hyperparameter that can be tuned to optimize the performance of the model. It can be any positive integer, and does not necessarily have to be odd or even. However, choosing an appropriate value of k is important to avoid underfitting or overfitting the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a57a08-0872-48d1-98b9-cf072369fe09",
   "metadata": {},
   "source": [
    "#### Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "- in classifier take k nearest points  and which one of category have more points thats category is our output.\n",
    "- In regressor take k nearest points and calculate the  average of that poits value is out predicted output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a42b75-a6a9-40bb-b145-febe2feb92a6",
   "metadata": {},
   "source": [
    "### Q4. How do you measure the performance of KNN?\n",
    "- Accurcacy , Precisin, Recall, F1 score\n",
    "- R2 score, mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a585b15c-4f72-494c-90a0-6473e7eafc0c",
   "metadata": {},
   "source": [
    "#### Q5. What is the curse of dimensionality in KNN?\n",
    "- The curse of dimensionality is a phenomenon that occurs in KNN (k-nearest neighbors) and other machine learning algorithms when the number of features or dimensions in the data is high. It refers to the fact that as the number of dimensions increases, the amount of data needed to generalize accurately to new data grows exponentially."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0075266-aca4-46ac-8b77-4c20da5108a9",
   "metadata": {},
   "source": [
    "#### Q6. How do you handle missing values in KNN?\n",
    "- Each sample's missing values are imputed using the mean value of the 'k'-neighbors found in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43879e8d-6ec8-4efc-86ca-d2c433d2209a",
   "metadata": {},
   "source": [
    "#### Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "\n",
    "- Performance: KNN classifier and KNN regressor have different evaluation metrics. The classifier is evaluated based on metrics such as accuracy, precision, recall, and F1-score, while the regressor is evaluated based on metrics such as mean squared error, mean absolute error, and R-squared. Therefore, the performance of KNN classifier and KNN regressor cannot be compared directly.\n",
    "\n",
    "- Type of problem: KNN classifier is suitable for classification problems where the goal is to predict the class labels of new instances based on the labels of the k-nearest neighbors. KNN regressor, on the other hand, is suitable for regression problems where the goal is to predict a continuous target variable based on the values of the k-nearest neighbors.\n",
    "\n",
    "- Data distribution: KNN classifier and KNN regressor perform well when the training data is evenly distributed across the feature space. However, in cases where the data is unevenly distributed or has outliers, KNN regressor may perform poorly.\n",
    "\n",
    "- Parameter tuning: The choice of the k parameter in KNN has a significant impact on the performance of the algorithm. In KNN classifier, a smaller value of k can reduce overfitting, while in KNN regressor, a larger value of k can improve the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1b23a9-8723-4ab1-badd-7130604cd3be",
   "metadata": {},
   "source": [
    "#### Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "- Strengths of KNN:\n",
    "\n",
    "    -Simple implementation: KNN is a simple and easy-to-understand algorithm that can be implemented with just a few lines of code.\n",
    "    \n",
    "    -Non-parametric: KNN is a non-parametric algorithm, which means it makes no assumptions about the distribution of the data.\n",
    "    \n",
    "    -Versatile: KNN can be used for both classification and regression tasks.\n",
    "    \n",
    "    -Effective with small datasets: KNN is effective when the dataset is small, and the number of features is relatively low.\n",
    "    \n",
    "- Weaknesses of KNN:\n",
    "\n",
    "    - Sensitive to noise: KNN is sensitive to noisy data, outliers, and irrelevant features. It can lead to overfitting and poor performance if the dataset is noisy or contains irrelevant features.\n",
    "\n",
    "    - Computationally expensive: KNN can be computationally expensive, especially when the dataset is large or the number of features is high. Computing the distances between all pairs of instances can be time-consuming and resource-intensive.\n",
    "\n",
    "    - Requires careful parameter tuning: The choice of the k parameter in KNN can significantly impact the performance of the algorithm. The optimal value of k depends on the nature of the data and the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a469f9c-c09b-4e28-8f56-cb59dc95dd67",
   "metadata": {},
   "source": [
    "#### Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "- Euclidean distance is calculated as the straight-line distance between two points in Euclidean space. It is the square root of the sum of the squared differences between the coordinates of the two points. On the other hand, Manhattan distance is calculated as the sum of the absolute differences between the coordinates of the two points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1c11a4-327d-498c-a24a-007a035e5818",
   "metadata": {},
   "source": [
    "#### Q10. What is the role of feature scaling in KNN?\n",
    "- Without feature scaling, features with large values can dominate the distance calculation and make the algorithm biased towards those features.\n",
    "-feature scaling is an important preprocessing step in KNN algorithm to ensure that all features are on the same scale and contribute equally to the distance calculation. This can help to reduce bias and improve the accuracy of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a505d5aa-936d-4640-a41a-f91efa79db38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
